import os
import spacy
import string
import unidecode
from collections import Counter

def tuples_to_dicts(keys, list_of_tuples):
    return [dict(zip(keys, values)) for values in list_of_tuples]

class Document:

    def __init__(self, file_path):
        # The name of the file is obtained
        self._file_name = os.path.basename(file_path)
        # Spacy features to process data in English are loaded
        nlp = spacy.load('en')
        # The content of the text file is stored
        raw_text = open(file_path, 'r').read()
        # Spacy is applied to get a data structure already featurized
        spacy_raw_text = nlp(raw_text)
        # Each sentence is stored on a list
        self._sentences = [sentence.string.strip() for sentence in spacy_raw_text.sents]
        # The text is decoded to map uncommon characters to common ones, then transformed into lowercase, and finally punctuation is removed
        self._processed_text = nlp(unidecode.unidecode(raw_text).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))))

    def get_most_common_words(self):
        return self._common_words_and_freq

    def determine_most_common_words(self, amount_of_words):
        # If this method is invoked more than once, the value of _common_words_and_freq is calculated only once
        if not hasattr(self, '_common_words_and_freq'):
            # The words found on the text are stored
            words = [chain.text for chain in self._processed_text if chain.pos_ == 'NOUN']
            # The frequency of each word is stored with it, as a list of tuples and the 'N' most frequent ones are taken
            self._common_words_and_freq = Counter(words).most_common(amount_of_words)
            # The list of tuples generated by Counter, is transformed to a list of dictionaries
            self._common_words_and_freq = tuples_to_dicts(['value', 'frequency'], self._common_words_and_freq)
            # Third and fourth components are added to the dictionary,
            # to store the name of the document where the word appear, and the presence of the word on each sentence, as a list of indexes
            for word_index, word in enumerate(self._common_words_and_freq):
                self._common_words_and_freq[word_index]['file'] = self._file_name
                self._common_words_and_freq[word_index]['sentences'] = []

    def assign_sentences_to_words(self):
        # If the most common words were determined and linked to the sentences where they appear,
        # these calculations are not going to be executed again
        if hasattr(self, '_common_words_and_freq') and not hasattr(self, '_processed_sentence'):
            # Spacy features used to process data in English are loaded
            nlp = spacy.load('en')
            # For each sentence, it determines the existence of each of the words
            for sentence_index, sentence in enumerate(self._sentences):
                # The text is decoded to map uncommon characters to common ones, then transformed into lowercase, and finally punctuation is removed
                self._processed_sentence = nlp(unidecode.unidecode(sentence).lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))))
                for word_index, word in enumerate(self._common_words_and_freq):
                    if word['value'] in [chain.text for chain in self._processed_sentence if chain.pos_ == 'NOUN']:
                        # If the the word exist on the sentence, sentence's location on the text is stored on the list of sentences of each word
                        self._common_words_and_freq[word_index]['sentences'].append(sentence_index)
